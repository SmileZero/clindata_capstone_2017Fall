{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the packages \n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load required pkl and csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = pd.read_csv(\"FAERS_columnSubset_Group_1.csv\")\n",
    "group2 = pd.read_csv(\"FAERS_columnSubset_Group_2.csv\")\n",
    "group3 = pd.read_csv(\"FAERS_columnSubset_Group_3.csv\")\n",
    "group4 = pd.read_csv(\"FAERS_columnSubset_Group_4.csv\")\n",
    "group5 = pd.read_csv(\"FAERS_columnSubset_Group_5.csv\")\n",
    "group6 = pd.read_csv(\"FAERS_columnSubset_Group_6.csv\")\n",
    "group7 = pd.read_csv(\"FAERS_columnSubset_Group_7.csv\")\n",
    "group8 = pd.read_csv(\"FAERS_columnSubset_Group_8.csv\")\n",
    "group9 = pd.read_csv(\"FAERS_columnSubset_Group_9.csv\")\n",
    "group10 = pd.read_csv(\"FAERS_columnSubset_Group_10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the unique_dict_categorical file for unique values for the feature matrix \n",
    "file_name = open(\"unique_categorical_dict.pkl\", \"r\")\n",
    "unique_categorical_dict = pickle.load(file_name)\n",
    "file_name.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the  file for unique values for the reaction, create both a dictionary and a list. \n",
    "top2000Reactions = pd.read_csv(\"Top2000Reactions.csv\")\n",
    "unique_react_list = top2000Reactions['Reaction'].unique()\n",
    "reactions_set = set(top2000Reactions.Reaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneHotEncoding and split the feature matrix(X) and reactions(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create function to convert to dummy variables\n",
    "def convert_to_dummies(data, categorical_columns, unique_dictionary):\n",
    "    #Assign the categories for each column\n",
    "    for column in categorical_columns:\n",
    "        data[column] = data[column].astype('category', categories=unique_dictionary[column])\n",
    "    #Convert to categorical\n",
    "    dummy = pd.get_dummies(data, columns=categorical_columns)\n",
    "    return dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to convert to dummy a specific group, only the feature matrix, the reaction will keep the same \n",
    "categorical_columns = ['RxCUI', 'indications', 'role_cod', 'sex', 'age_grp', \n",
    "                       'dechal', 'rechal']\n",
    "group1_dummy = convert_to_dummies(group1, categorical_columns, unique_categorical_dict)\n",
    "group2_dummy = convert_to_dummies(group2, categorical_columns, unique_categorical_dict)\n",
    "group3_dummy = convert_to_dummies(group3, categorical_columns, unique_categorical_dict)\n",
    "group4_dummy = convert_to_dummies(group4, categorical_columns, unique_categorical_dict)\n",
    "group5_dummy = convert_to_dummies(group5, categorical_columns, unique_categorical_dict)\n",
    "group6_dummy = convert_to_dummies(group6, categorical_columns, unique_categorical_dict)\n",
    "group7_dummy = convert_to_dummies(group7, categorical_columns, unique_categorical_dict)\n",
    "group8_dummy = convert_to_dummies(group8, categorical_columns, unique_categorical_dict)\n",
    "group9_dummy = convert_to_dummies(group9, categorical_columns, unique_categorical_dict)\n",
    "group10_dummy = convert_to_dummies(group10, categorical_columns, unique_categorical_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate select the reactions columns to create Y_value\n",
    "def separate_x_y(data):\n",
    "    data_x = data[data.columns.difference(['Unnamed: 0','Unnamed: 0.1','caseid','drug_seq','reactions'])]\n",
    "    data_y =data.filter(regex='reactions')\n",
    "    return data_x,data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the oneHotEncoding data\n",
    "group1_x,group1_y = separate_x_y(group1_dummy)\n",
    "group2_x,group2_y = separate_x_y(group2_dummy)\n",
    "group3_x,group3_y = separate_x_y(group3_dummy)\n",
    "group4_x,group4_y = separate_x_y(group4_dummy)\n",
    "group5_x,group5_y = separate_x_y(group5_dummy)\n",
    "group6_x,group6_y = separate_x_y(group6_dummy)\n",
    "group7_x,group7_y = separate_x_y(group7_dummy)\n",
    "group8_x,group8_y = separate_x_y(group8_dummy)\n",
    "group9_x,group9_y = separate_x_y(group9_dummy)\n",
    "group10_x,group10_y = separate_x_y(group10_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape both the X and Y, Save all as Pickled Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshape the Feature Matrix \n",
    "# the funcation to reshape the data format, size is the number of the group, group is the data\n",
    "def splitX(size, group):\n",
    "    caseid_data = []\n",
    "    group_size = len(group)/size\n",
    "    for case_order in range(group_size):\n",
    "        row = group.loc[(case_order*size):(case_order*size+(size-1))].values\n",
    "        newrow = scipy.sparse.csc_matrix(row)\n",
    "        caseid_data.append(newrow)\n",
    "    return caseid_data\n",
    "\n",
    "def split_coma(s):\n",
    "    return s.split('\\\\,')\n",
    "\n",
    "#Remove the reactions that are not in the set of 2000\n",
    "def reaction_intersection(reaction_list):\n",
    "    return list(reactions_set.intersection(reaction_list))\n",
    "\n",
    "def splitY(size,group):\n",
    "    reactions_splited = pd.DataFrame()\n",
    "    reactions_splited['reaction'] = map(split_coma, group10_y['reactions'])\n",
    "    y_splited = pd.DataFrame()\n",
    "    y_splited['reaction'] = map(reaction_intersection,reactions_splited.reaction)\n",
    "    mlb= MultiLabelBinarizer(classes=unique_react_list)\n",
    "    group_formated = mlb.fit_transform(y_splited['reaction'])\n",
    "    caseid_data = []\n",
    "    group_size = len(group_formated)/size\n",
    "    for case_order in range(group_size):\n",
    "        row = group_formated[(case_order*size):(case_order*size+size)]\n",
    "        newrow = scipy.sparse.csc_matrix(row)\n",
    "        caseid_data.append(newrow)\n",
    "    return caseid_data\n",
    "\n",
    "def savePickledFile(name,data):\n",
    "    with open (name,\"wb\") as fp:\n",
    "        pickle.dump(data,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the feature matrix for different groups into pickled File \n",
    "group1_x_sparse = splitX(1,group1_x)\n",
    "# savePickledFile(\"group1_x_sparse.pkl\",group1_x_sparse)\n",
    "group1_y_sparse = splitY(1,group1_y)\n",
    "savePickledFile(\"group1_y_sparse.pkl\",group1_y_sparse)    \n",
    "\n",
    "group2_x_sparse = splitX(2,group2_x)\n",
    "savePickledFile(\"group2_x_sparse.pkl\",group2_x_sparse)\n",
    "group2_y_sparse = splitY(2,group2_y)\n",
    "savePickledFile(\"group2_y_sparse.pkl\",group2_y_sparse)  \n",
    "\n",
    "group3_x_sparse = splitX(3,group3_x)\n",
    "savePickledFile(\"group3_x_sparse.pkl\",group3_x_sparse)\n",
    "group3_y_sparse = splitY(3,group3_y)\n",
    "savePickledFile(\"group3_x_sparse.pkl\",group3_x_sparse)  \n",
    "\n",
    "group4_x_sparse = splitX(4,group4_x)\n",
    "savePickledFile(\"group4_x_sparse.pkl\",group4_x_sparse)\n",
    "group4_y_sparse = splitY(4,group4_y)\n",
    "savePickledFile(\"group4_y_sparse.pkl\",group4_y_sparse)  \n",
    "\n",
    "group5_x_sparse = splitX(5,group5_x)\n",
    "savePickledFile(\"group5_x_sparse.pkl\",group5_x_sparse)\n",
    "group5_y_sparse = splitY(5,group5_y)\n",
    "savePickledFile(\"group5_y_sparse.pkl\",group5_y_sparse)  \n",
    "\n",
    "group6_x_sparse = splitX(6,group6_x)\n",
    "savePickledFile(\"group6_x_sparse.pkl\",group6_x_sparse)\n",
    "group6_y_sparse = splitY(6,group6_y)\n",
    "savePickledFile(\"group6_y_sparse.pkl\",group6_y_sparse) \n",
    "\n",
    "group7_x_sparse = splitX(7,group7_x)\n",
    "savePickledFile(\"group7_x_sparse.pkl\",group7_x_sparse)\n",
    "group7_y_sparse = splitY(7,group7_y)\n",
    "savePickledFile(\"group7_y_sparse.pkl\",group7_y_sparse) \n",
    "\n",
    "group8_x_sparse = splitX(8,group8_x)\n",
    "savePickledFile(\"group8_x_sparse.pkl\",group8_x_sparse)\n",
    "group8_y_sparse = splitY(8,group8_y)\n",
    "savePickledFile(\"group8_y_sparse.pkl\",group8_y_sparse) \n",
    "\n",
    "group9_x_sparse = splitX(9,group9_x)\n",
    "savePickledFile(\"group9_x_sparse.pkl\",group9_x_sparse)\n",
    "group9_y_sparse = splitY(9,group9_y)\n",
    "savePickledFile(\"group9_y__sparse.pkl\",group9_y_sparse)\n",
    "\n",
    "group10_x_formated = splitX(10,group10_x)\n",
    "savePickledFile(\"group10_x_formated.pkl\",group10_x_formated)\n",
    "group10_y_formated = splitY(10,group10_y)\n",
    "savePickledFile(\"group10_y_formated.pkl\",group10_y_formated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10L, 9321L)\n",
      "(10L, 2000L)\n"
     ]
    }
   ],
   "source": [
    "print(group10_x_sparse[2].todense().shape)\n",
    "print(group10_y_sparse[2].todense().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 9321)\n",
      "(10, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(group10_x_sparse[2].shape)\n",
    "print(group10_y_sparse[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check header \n",
    "# print (list(mlb.classes_))\n",
    "# print (mlb.inverse_transform(group_formated)[1])\n",
    "# np.where(group_formated[1]==1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
