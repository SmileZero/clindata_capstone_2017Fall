{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the packages \n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load required pkl and csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group1 = pd.read_csv(\"ColumnSubsets\\FAERS_columnSubset_Group_1.csv\")\n",
    "# group2 = pd.read_csv(\"ColumnSubsets\\FAERS_columnSubset_Group_2.csv\")\n",
    "# group3 = pd.read_csv(\"ColumnSubsets\\FAERS_columnSubset_Group_3.csv\")\n",
    "# group4 = pd.read_csv(\"ColumnSubsets\\FAERS_columnSubset_Group_4.csv\")\n",
    "# group5 = pd.read_csv(\"ColumnSubsets\\FAERS_columnSubset_Group_5.csv\")\n",
    "# group6 = pd.read_csv(\"ColumnSubsets\\FAERS_columnSubset_Group_6.csv\")\n",
    "# group7 = pd.read_csv(\"ColumnSubsets\\FAERS_columnSubset_Group_7.csv\")\n",
    "# group8 = pd.read_csv(\"ColumnSubsets\\FAERS_columnSubset_Group_8.csv\")\n",
    "group9 = pd.read_csv(\"ColumnSubsets\\FAERS_columnSubset_Group_9.csv\")\n",
    "#group10 = pd.read_csv(\"ColumnSubsets\\FAERS_columnSubset_Group_10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the unique_dict_categorical file for unique values for the feature matrix \n",
    "file_name = open(\"unique_categorical_dict.pkl\", \"r\")\n",
    "unique_categorical_dict = pickle.load(file_name)\n",
    "file_name.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the  file for unique values for the reaction\n",
    "unique_react = pd.read_csv(\"distinct_reactions.csv\")\n",
    "unique_react_list = unique_react['class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneHotEncoding and split the feature matrix(X) and reactions(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create function to convert to dummy variables\n",
    "def convert_to_dummies(data, categorical_columns, unique_dictionary):\n",
    "    #Assign the categories for each column\n",
    "    for column in categorical_columns:\n",
    "        data[column] = data[column].astype('category', categories=unique_dictionary[column])\n",
    "    #Convert to categorical\n",
    "    dummy = pd.get_dummies(data, columns=categorical_columns)\n",
    "    return dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to convert to dummy a specific group, only the feature matrix, the reaction will keep the same \n",
    "categorical_columns = ['RxCUI', 'indications', 'role_cod', 'sex', 'age_grp', \n",
    "                       'dechal', 'rechal']\n",
    "# group1_dummy = convert_to_dummies(group1, categorical_columns, unique_categorical_dict)\n",
    "# group2_dummy = convert_to_dummies(group2, categorical_columns, unique_categorical_dict)\n",
    "# group3_dummy = convert_to_dummies(group3, categorical_columns, unique_categorical_dict)\n",
    "#group4_dummy = convert_to_dummies(group4, categorical_columns, unique_categorical_dict)\n",
    "# group5_dummy = convert_to_dummies(group5, categorical_columns, unique_categorical_dict)\n",
    "# group6_dummy = convert_to_dummies(group6, categorical_columns, unique_categorical_dict)\n",
    "# group7_dummy = convert_to_dummies(group7, categorical_columns, unique_categorical_dict)\n",
    "# group8_dummy = convert_to_dummies(group8, categorical_columns, unique_categorical_dict)\n",
    "group9_dummy = convert_to_dummies(group9, categorical_columns, unique_categorical_dict)\n",
    "#group10_dummy = convert_to_dummies(group10, categorical_columns, unique_categorical_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate select the reactions columns to create Y_value\n",
    "def separate_x_y(data):\n",
    "    data_x = data[data.columns.difference(['Unnamed: 0','Unnamed: 0.1','caseid','drug_seq','reactions'])]\n",
    "    data_y =data.filter(regex='reactions')\n",
    "    return data_x,data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the oneHotEncoding data\n",
    "# group1_x,group1_y = separate_x_y(group1_dummy)\n",
    "# group2_x,group2_y = separate_x_y(group2_dummy)\n",
    "# group3_x,group3_y = separate_x_y(group3_dummy)\n",
    "# group4_x,group4_y = separate_x_y(group4_dummy)\n",
    "# group5_x,group5_y = separate_x_y(group5_dummy)\n",
    "# group6_x,group6_y = separate_x_y(group6_dummy)\n",
    "# group7_x,group7_y = separate_x_y(group7_dummy)\n",
    "# group8_x,group8_y = separate_x_y(group8_dummy)\n",
    "group9_x,group9_y = separate_x_y(group9_dummy)\n",
    "#group10_x,group10_y = separate_x_y(group10_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape both the X and Y, Save all as Pickled Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshape the Feature Matrix \n",
    "# the funcation to reshape the data format, size is the number of the group, group is the data\n",
    "def splitX(size, group):\n",
    "    caseid_data = []\n",
    "    group_size = len(group)/size\n",
    "    for case_order in range(group_size):\n",
    "        row = group.loc[(case_order*size):(case_order*size+(size-1))].values\n",
    "        caseid_data.append(row)\n",
    "    return caseid_data\n",
    "\n",
    "# Reshape the Y table\n",
    "def split_coma(s):\n",
    "    return s.split('\\\\,')\n",
    "\n",
    "def splitY(size,group):\n",
    "    reactions_splited = map(split_coma, group['reactions'])\n",
    "    mlb = MultiLabelBinarizer(classes=unique_react_list)  # oneHotEncoding function\n",
    "    #Transform on each group\n",
    "    group_formated = mlb.fit_transform(reactions_splited)\n",
    "    print (mlb.inverse_transform(group_formated))\n",
    "    caseid_data = []\n",
    "    group_size = len(group_formated)/size\n",
    "    for case_order in range(group_size):\n",
    "        row = group_formated[(case_order*size):(case_order*size+size)]\n",
    "        caseid_data.append(row)\n",
    "    return caseid_data\n",
    "\n",
    "def savePickledFile(name,data):\n",
    "    with open (name,\"wb\") as fp:\n",
    "        pickle.dump(data,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save all the feature matrix for different groups into pickled File \n",
    "# group1_x_formated = splitX(1,group1_x)\n",
    "# savePickledFile(\"group1_x_formated.pkl\",group1_x_formated)\n",
    "# group1_y_formated = splitY(1,group1_y)\n",
    "# savePickledFile(\"group1_y_formated.pkl\",group1_y_formated)    \n",
    "\n",
    "# group2_x_formated = splitX(2,group2_x)\n",
    "# savePickledFile(\"group2_x_formated.pkl\",group2_x_formated)\n",
    "# group2_y_formated = splitY(2,group2_y)\n",
    "# savePickledFile(\"group2_y_formated.pkl\",group2_y_formated)  \n",
    "\n",
    "# group3_x_formated = splitX(3,group3_x)\n",
    "# savePickledFile(\"group3_x_formated.pkl\",group3_x_formated)\n",
    "# group3_y_formated = splitY(3,group3_y)\n",
    "# savePickledFile(\"group3_x_formated.pkl\",group3_x_formated)  \n",
    "\n",
    "# group4_x_formated = splitX(4,group4_x)\n",
    "# savePickledFile(\"group4_x_formated.pkl\",group4_x_formated)\n",
    "# group4_y_formated = splitY(4,group4_y)\n",
    "# savePickledFile(\"group4_y_formated.pkl\",group4_y_formated)  \n",
    "\n",
    "# group5_x_formated = splitX(5,group5_x)\n",
    "# savePickledFile(\"group5_x_formated.pkl\",group5_x_formated)\n",
    "# group5_y_formated = splitY(5,group5_y)\n",
    "# savePickledFile(\"group5_y_formated.pkl\",group5_y_formated)  \n",
    "\n",
    "# group6_x_formated = splitX(6,group6_x)\n",
    "# savePickledFile(\"group6_x_formated.pkl\",group6_x_formated)\n",
    "# group6_y_formated = splitY(6,group6_y)\n",
    "# savePickledFile(\"group6_y_formated.pkl\",group6_y_formated) \n",
    "\n",
    "# group7_x_formated = splitX(7,group7_x)\n",
    "# savePickledFile(\"group7_x_formated.pkl\",group7_x_formated)\n",
    "# group7_y_formated = splitY(7,group7_y)\n",
    "# savePickledFile(\"group7_y_formated.pkl\",group7_y_formated) \n",
    "\n",
    "# group8_x_formated = splitX(8,group8_x)\n",
    "# savePickledFile(\"group8_x_formated.pkl\",group8_x_formated)\n",
    "# group8_y_formated = splitY(8,group8_y)\n",
    "# savePickledFile(\"group8_y_formated.pkl\",group8_y_formated) \n",
    "\n",
    "# group9_x_formated = splitX(9,group9_x)\n",
    "# savePickledFile(\"group9_x_formated.pkl\",group9_x_formated)\n",
    "# group9_y_formated = splitY(9,group9_y)\n",
    "# savePickledFile(\"group9_y__formated.pkl\",group9_y_formated)\n",
    "\n",
    "group10_x_formated = splitX(10,group10_x)\n",
    "savePickledFile(\"group10_x_formated.pkl\",group10_x_formated)\n",
    "group10_y_formated = splitY(10,group10_y)\n",
    "savePickledFile(\"group10_y_formated.pkl\",group10_y_formated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "reactions_splited = map(split_coma, group9_y['reactions'])\n",
    "mlb = MultiLabelBinarizer(classes=unique_react_list)  # oneHotEncoding function\n",
    "    #Transform on each group\n",
    "group_formated = mlb.fit_transform(reactions_splited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('PAPILLARY THYROID CANCER', 'GOITRE')\n"
     ]
    }
   ],
   "source": [
    "#print(unique_react_list.dtype.names)\n",
    "print (mlb.inverse_transform(group_formated)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4069, 7273], dtype=int64),)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(group_formated[1]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOITRE\n"
     ]
    }
   ],
   "source": [
    "print (list(mlb.classes_)[7273])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
